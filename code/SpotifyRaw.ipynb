{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7774f6a2-87bf-4c95-a846-7afbe65ba150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2185a3e6-d594-4f9e-96cd-1e7ffaeb529a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.io.directoryCommit.createSuccessFile\",\"false\") \n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f9076a-bebc-4b36-99ae-08166a138bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the mount point already exists\n",
    "mount_point = \"/mnt/spotifydata\"\n",
    "mounts = dbutils.fs.mounts()\n",
    "mount_exists = any(mount.mountPoint == mount_point for mount in mounts)\n",
    "\n",
    "if not mount_exists:\n",
    "  # get secret from Azure Key Vault\n",
    "  adls_client_id = dbutils.secrets.get(scope = \"secret-store\", key = \"adls-spotify-client-id\")\n",
    "  adls_client_secret = dbutils.secrets.get(scope = \"secret-store\", key = \"adls-spotify-client-secret\")\n",
    "  adls_tenant_id = dbutils.secrets.get(scope = \"secret-store\", key = \"adls-spotify-tenant-id\")\n",
    "\n",
    "  configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  \"fs.azure.account.oauth2.client.id\": adls_client_id,\n",
    "  \"fs.azure.account.oauth2.client.secret\": adls_client_secret,\n",
    "  \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{adls_tenant_id}/oauth2/token\"}\n",
    "\n",
    "  dbutils.fs.mount(\n",
    "    source = \"abfss://spotifydata@spotifyproject888.dfs.core.windows.net\", # contrainer@storageacc\n",
    "    mount_point = mount_point,\n",
    "    extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9a7919-3845-42e8-a309-a99573b2f7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get secret from Azure Key Vault\n",
    "client_id = dbutils.secrets.get(scope = \"secret-store\", key = \"spotify-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope = \"secret-store\", key = \"spotify-client-secret\")\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager, requests_timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3a65c1-e9c4-4616-974d-bb03bfea86cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0425ed-1222-49f7-9b23-0c6de7e9c80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get tracks in the current year\n",
    "track_list=[]\n",
    "for i in range(0,1000,50):\n",
    "  track_result = sp.search(q = f'year:{current_year}', type=\"track\", limit=50, offset=i, market='US')\n",
    "  for row in track_result['tracks']['items']:\n",
    "      artist_id = row['artists'][0]['id'] \n",
    "      track_id = row['id']\n",
    "      track_name = row['name']\n",
    "      track_popularity = row['popularity']\n",
    "      track_release_date = row['album']['release_date']\n",
    "      track_list.append({'artist_id':artist_id, 'track_id':track_id, 'track_name':track_name, 'track_popularity':track_popularity, 'track_release_date':track_release_date})\n",
    "track_df = pd.DataFrame.from_dict(track_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1678d02-32a1-4a84-9506-9f22dc83a7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get artist from the track\n",
    "artist_list = []\n",
    "artist_id_list = track_df['artist_id'].to_list()\n",
    "artist_id_set = set(artist_id_list)\n",
    "artist_id_list_unique = list(artist_id_set)\n",
    "\n",
    "for row in artist_id_list_unique:\n",
    "  artist_result = sp.artist(row)\n",
    "  artist_id = artist_result['id']\n",
    "  artist_name = artist_result['name']\n",
    "  artist_popularity = artist_result['popularity']\n",
    "  artist_follower = artist_result['followers']['total']\n",
    "  if len(artist_result['images']) == 0:\n",
    "    artist_image = None\n",
    "  else:\n",
    "    artist_image = artist_result['images'][0]['url'] \n",
    "  artist_list.append({'artist_id':artist_id, 'artist_name': artist_name , 'artist_popularity':artist_popularity, 'artist_follower':artist_follower, 'artist_image':artist_image})\n",
    "\n",
    "artist_df = pd.DataFrame.from_dict(artist_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46522e82-2bf7-4703-8f20-1d7449581796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to spark dataframe \n",
    "track_spark_df = spark.createDataFrame(track_df)\n",
    "artist_spark_df = spark.createDataFrame(artist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252e8fe2-20c0-42ce-a5a3-d07d08625f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "today_date = datetime.now().strftime('%Y%m%d')  # Format: yyyymmdd\n",
    "\n",
    "track_folder = \"/mnt/spotifydata/to_be_processed/track\"\n",
    "artist_folder = \"/mnt/spotifydata/to_be_processed/artist\"\n",
    "track_file_name = f\"track_{today_date}.parquet\"\n",
    "artist_file_name = f\"artist_{today_date}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8322c3ea-d38d-4c8d-930d-b412bb978ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Write the DataFrame to the output folder (coalescing to a single file)\n",
    "track_spark_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(track_folder)\n",
    "          \n",
    "artist_spark_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(artist_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ceb1034-99ff-4a30-9d6e-a8e3020e2242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_parquet_file(folder_path, target_file_name):\n",
    "\n",
    "  # Step 2: List all files in the output folder\n",
    "  files = dbutils.fs.ls(folder_path)\n",
    "\n",
    "  # Step 3: Identify the Parquet file generated by Spark (starts with 'part')\n",
    "  parquet_file = [file.path for file in files if file.name.startswith(\"part\") and file.name.endswith(\".parquet\")]\n",
    "\n",
    "  if parquet_file:\n",
    "    # Step 4: Rename the Parquet file to the desired name\n",
    "    dbutils.fs.mv(parquet_file[0], f\"{folder_path}/{target_file_name}\")\n",
    "    \n",
    "     # Step 5: Remove any other system-generated files (e.g., _SUCCESS)\n",
    "    for file in files:\n",
    "        if file.name != target_file_name:\n",
    "            dbutils.fs.rm(file.path, recurse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7a3862-d374-400e-8fe1-f509f455954f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rename_parquet_file(track_folder, track_file_name)\n",
    "rename_parquet_file(artist_folder, artist_file_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SpotifyRaw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
